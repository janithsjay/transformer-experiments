{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVsHFh80YjzFwBL/bN+cB1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janithsjay/transformer-experiments/blob/main/tiny_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "wFuXlirI721b"
      },
      "outputs": [],
      "source": [
        "# mini_transformer.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Step 1: Tiny Dataset\n",
        "# ----------------------\n",
        "sentences = [\n",
        "    \"I like pizza\",\n",
        "    \"I like cats\",\n",
        "    \"I took the dog for a walk\",\n",
        "    \"The sun is bright\",\n",
        "    \"I went to the park\",\n",
        "    \"The dog likes to play\",\n",
        "    \"I love dogs\",\n",
        "    \"dogs are lovely\"\n",
        "]"
      ],
      "metadata": {
        "id": "eCiOQOBq7_94"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary\n",
        "tokens = set()\n",
        "for sentence in sentences:\n",
        "    tokens.update(sentence.lower().split())\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veoDtBCo8GL1",
        "outputId": "45c95529-4e46-4237-8af8-0fccb6d139bd"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'cats', 'took', 'sun', 'the', 'like', 'is', 'love', 'a', 'likes', 'park', 'went', 'lovely', 'for', 'are', 'dog', 'dogs', 'walk', 'to', 'bright', 'i', 'pizza', 'play'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token2id = {tok: idx for idx, tok in enumerate(sorted(tokens))}\n",
        "id2token = {idx: tok for tok, idx in token2id.items()}\n",
        "vocab_size = len(token2id)\n",
        "print(token2id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy4WZx148NnV",
        "outputId": "c384b4cf-1b59-4e97-fc0c-639abb691e00"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 0, 'are': 1, 'bright': 2, 'cats': 3, 'dog': 4, 'dogs': 5, 'for': 6, 'i': 7, 'is': 8, 'like': 9, 'likes': 10, 'love': 11, 'lovely': 12, 'park': 13, 'pizza': 14, 'play': 15, 'sun': 16, 'the': 17, 'to': 18, 'took': 19, 'walk': 20, 'went': 21}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sentences to sequences of IDs\n",
        "sequences = [[token2id[word] for word in sentence.lower().split()] for sentence in sentences]\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf7sZzRI8Wib",
        "outputId": "93eb13bd-642f-473d-a233-c5a016237427"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[7, 9, 14], [7, 9, 3], [7, 19, 17, 4, 6, 0, 20], [17, 16, 8, 2], [7, 21, 18, 17, 13], [17, 4, 10, 18, 15], [7, 11, 5], [5, 1, 12]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input/output pairs for next-word prediction\n",
        "X, Y = [], []\n",
        "for seq in sequences:\n",
        "    for i in range(1, len(seq)):\n",
        "        X.append(seq[:i])\n",
        "        Y.append(seq[i])\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ngV6OXSD8esg"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Positional Encoding\n",
        "\n",
        "In transformers, unlike RNNs, the model doesn‚Äôt inherently know the order of tokens in a sequence. To give it a sense of position, we add positional encodings to the input embeddings. These encodings help the model distinguish between the first word, second word, etc.\n",
        "\n",
        "Here‚Äôs what your code does:\n",
        "\n",
        "Parameters:\n",
        "\n",
        "d_model = 16 ‚Üí The dimensionality of the embeddings (number of features per token).\n",
        "\n",
        "max_len = 10 ‚Üí Maximum length of the sequence we want to encode positions for.\n",
        "\n",
        "Function get_positional_encoding(seq_len, d_model):\n",
        "\n",
        "Creates a zero tensor of shape (seq_len, d_model) to store the positional encodings.\n",
        "\n",
        "Loops over each position in the sequence (pos) and each dimension of the embedding (i).\n",
        "\n",
        "Even indices (i): Use the sine function\n",
        "\n",
        "ùëÉ\n",
        "ùê∏\n",
        "[\n",
        "ùëù\n",
        "ùëú\n",
        "ùë†\n",
        ",\n",
        "ùëñ\n",
        "]\n",
        "=\n",
        "sin\n",
        "‚Å°\n",
        "(\n",
        "ùëù\n",
        "ùëú\n",
        "ùë†\n",
        "10000\n",
        "ùëñ\n",
        "/\n",
        "ùëë\n",
        "_\n",
        "ùëö\n",
        "ùëú\n",
        "ùëë\n",
        "ùëí\n",
        "ùëô\n",
        ")\n",
        "PE[pos,i]=sin(\n",
        "10000\n",
        "i/d_model\n",
        "pos\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "\n",
        "Odd indices (i+1): Use the cosine function\n",
        "\n",
        "ùëÉ\n",
        "ùê∏\n",
        "[\n",
        "ùëù\n",
        "ùëú\n",
        "ùë†\n",
        ",\n",
        "ùëñ\n",
        "+\n",
        "1\n",
        "]\n",
        "=\n",
        "cos\n",
        "‚Å°\n",
        "(\n",
        "ùëù\n",
        "ùëú\n",
        "ùë†\n",
        "10000\n",
        "ùëñ\n",
        "/\n",
        "ùëë\n",
        "_\n",
        "ùëö\n",
        "ùëú\n",
        "ùëë\n",
        "ùëí\n",
        "ùëô\n",
        ")\n",
        "PE[pos,i+1]=cos(\n",
        "10000\n",
        "i/d_model\n",
        "pos\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "\n",
        "Why sine and cosine?\n",
        "\n",
        "They create a unique pattern for each position across all embedding dimensions.\n",
        "\n",
        "These patterns are continuous, so the model can infer the relative distances between positions.\n",
        "\n",
        "Using different frequencies (10000^(i/d_model)) ensures each dimension has a different periodicity.\n",
        "\n",
        "Output:\n",
        "\n",
        "pos_encoding is a tensor of shape (max_len, d_model).\n",
        "\n",
        "This tensor is later added to the input embeddings to inject positional information.\n",
        "\n",
        "üí° Intuition:\n",
        "Think of positional encoding as giving each token a ‚Äúlocation tag‚Äù in the sequence. Sine and cosine allow the model to figure out relative positions without hard-coding numbers."
      ],
      "metadata": {
        "id": "Z2gZ0wUN_mMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Step 2: Positional Encoding\n",
        "# ----------------------\n",
        "d_model = 16\n",
        "# max_len = 10 # Removed static max_len\n",
        "\n",
        "# Calculate max_len dynamically based on the longest sentence\n",
        "max_len = max(len(s.split()) for s in sentences)\n",
        "print(f\"Dynamic max_len: {max_len}\")\n",
        "\n",
        "def get_positional_encoding(seq_len, d_model):\n",
        "    pe = torch.zeros(seq_len, d_model)\n",
        "    for pos in range(seq_len):\n",
        "        for i in range(0, d_model, 2):\n",
        "            pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n",
        "            if i + 1 < d_model:\n",
        "                pe[pos, i+1] = math.cos(pos / (10000 ** (i / d_model)))\n",
        "    return pe\n",
        "\n",
        "# pos_encoding = get_positional_encoding(max_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABWALrPJ9VdU",
        "outputId": "3b1edd82-6007-4228-fad9-bd033341f429"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dynamic max_len: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Transformer Encoder Layer\n",
        "\n",
        "The transformer encoder layer is the core building block of a Transformer. It takes in a sequence of embeddings and outputs a transformed sequence that captures contextual relationships between tokens.\n",
        "\n",
        "Class Initialization (__init__)\n",
        "\n",
        "Linear projections for Q, K, V:\n",
        "\n",
        "self.W_q, self.W_k, self.W_v are linear layers that map the input embeddings into Query, Key, and Value vectors.\n",
        "\n",
        "Each has shape (d_model, d_model).\n",
        "\n",
        "These are the vectors used in self-attention to compute relationships between tokens.\n",
        "\n",
        "Feed-Forward Network (ffn):\n",
        "\n",
        "A small MLP applied independently to each position.\n",
        "\n",
        "Two linear layers: first expands the dimension by 4√ó (d_model -> 4*d_model), then reduces it back (4*d_model -> d_model).\n",
        "\n",
        "Uses ReLU for non-linearity.\n",
        "\n",
        "Layer Normalization (ln1, ln2):\n",
        "\n",
        "Helps stabilize training by normalizing the inputs at each step.\n",
        "\n",
        "Applied after residual connections."
      ],
      "metadata": {
        "id": "gAJjEL_KAE3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Step 3: Transformer Encoder Layer\n",
        "# ----------------------\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model*4, d_model)\n",
        "        )\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "        scores = Q @ K.T / math.sqrt(d_model)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_out = attn_weights @ V\n",
        "\n",
        "        x = self.ln1(x + attn_out)\n",
        "        x = self.ln2(x + self.ffn(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "KmbEoe0A9han"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Step 4: Mini Transformer\n",
        "# ----------------------\n",
        "class MiniTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = get_positional_encoding(max_len, d_model)\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer(d_model) for _ in range(num_layers)])\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, seq_ids):\n",
        "        seq_len = seq_ids.size(0)\n",
        "        x = self.embedding(seq_ids) + self.pos_encoding[:seq_len]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        logits = self.output_layer(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "VuabQcqz9k1X"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Step 5: Training Loop\n",
        "# ----------------------\n",
        "num_layers = 2\n",
        "learning_rate = 0.01\n",
        "model = MiniTransformer(vocab_size, d_model, num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train on tiny dataset\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for seq, target in zip(X, Y):\n",
        "        seq_ids = torch.tensor(seq)\n",
        "        target_id = torch.tensor([target])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(seq_ids)\n",
        "        pred = logits[-1].unsqueeze(0)\n",
        "        loss = criterion(pred, target_id)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsOcZdmg9ocF",
        "outputId": "5e1f3211-f9f0-4e45-e427-e2e54b2adca5"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 22.2382\n",
            "Epoch 20, Loss: 12.0126\n",
            "Epoch 30, Loss: 11.4462\n",
            "Epoch 40, Loss: 11.1547\n",
            "Epoch 50, Loss: 10.9612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Step 6: Test Prediction\n",
        "# ----------------------\n",
        "test_seq = torch.tensor([token2id[w] for w in [\"i\", \"like\"]])\n",
        "logits = model(test_seq)\n",
        "pred_id = logits[-1].argmax().item()\n",
        "print(\"Input: 'i like'\")\n",
        "print(\"Predicted next word:\", id2token[pred_id])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELRGXxkD9wB3",
        "outputId": "67b7dc0c-0545-4c97-917b-152e8d471522"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 'i like'\n",
            "Predicted next word: cats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Step 6: Test Prediction\n",
        "# ----------------------\n",
        "test_seq = torch.tensor([token2id[w] for w in [\"i\", \"took\", \"the\", \"cats\", \"for\", \"a\"]])\n",
        "logits = model(test_seq)\n",
        "pred_id = logits[-1].argmax().item()\n",
        "print(\"Predicted next word:\", id2token[pred_id])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8nn03Dj95v7",
        "outputId": "11b3b935-e124-48f3-ab78-73194d075baa"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted next word: walk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import sys\n",
        "\n",
        "# ----------------------\n",
        "# Step 6: Generate Sentence with \"pausing\"\n",
        "# ----------------------\n",
        "test_seq = [\"i\"]\n",
        "max_gen_len = 5  # maximum tokens to generate\n",
        "\n",
        "generated = test_seq.copy()\n",
        "\n",
        "# Print the starting sequence\n",
        "for word in test_seq:\n",
        "    print(word, end=\" \", flush=True)\n",
        "    time.sleep(0.3)  # small pause for \"thinking\"\n",
        "\n",
        "for _ in range(max_gen_len):\n",
        "    seq_ids = torch.tensor([token2id[w] for w in generated])\n",
        "    logits = model(seq_ids)\n",
        "    pred_id = logits[-1].argmax().item()\n",
        "    next_word = id2token[pred_id]\n",
        "\n",
        "    # Print next word immediately\n",
        "    print(next_word, end=\" \", flush=True)\n",
        "    time.sleep(0.3)  # pause between words\n",
        "\n",
        "    generated.append(next_word)\n",
        "\n",
        "    if next_word == \"<EOS>\":\n",
        "        break\n",
        "\n",
        "print()  # final newline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqKcZqNnOvvu",
        "outputId": "610091ac-550f-423b-86a8-b198a0513d10"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i like cats cats to the \n"
          ]
        }
      ]
    }
  ]
}