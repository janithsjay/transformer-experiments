{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMUNLiTAnAf3OhTac/aLLGD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janithsjay/transformer-experiments/blob/main/tiny_transformer_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wFuXlirI721b"
      },
      "outputs": [],
      "source": [
        "# mini_transformer.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"I like building small transformers to understand NLP\",\n",
        "    \"Debugging code is easier when logging is clear\",\n",
        "    \"I want to generate sentences token by token\",\n",
        "    \"Embedding vectors help words capture meaning in context\",\n",
        "    \"Positional encoding tells the model the order of words\",\n",
        "    \"I often experiment with PyTorch and Colab notebooks\",\n",
        "    \"GitHub integration makes version control and sharing easier\",\n",
        "    \"I test my models on tiny datasets first\",\n",
        "    \"Self attention lets each word look at other words\",\n",
        "    \"Feed forward networks refine token representations in transformers\",\n",
        "    \"I tweak d_model and layers to improve model capacity\",\n",
        "    \"Next-word prediction is the basis of text generation\",\n",
        "    \"I enjoy making small AI experiments that reflect my style\",\n",
        "    \"I like iterative loops to grow sentences one token at a time\",\n",
        "    \"Temperature and top-k sampling make outputs more creative\",\n",
        "    \"I analyze logs carefully when training fails or diverges\",\n",
        "    \"I optimize batch sizes and learning rates for efficiency\",\n",
        "    \"I explore synthetic sentences to enlarge tiny datasets\",\n",
        "    \"I want to build a mini AI version of myself\",\n",
        "    \"I focus on understanding transformers, embeddings, and self-attention\",\n",
        "    \"I sometimes write code to simulate thinking in generation loops\",\n",
        "    \"I use Jenkins and Docker to automate CI/CD pipelines\",\n",
        "    \"I like to experiment with short prompts and sequences\",\n",
        "    \"I debug runtime errors by checking tensor shapes\",\n",
        "    \"I test different learning rates to stabilize training\",\n",
        "    \"I enjoy building prototypes that actually produce sentences\",\n",
        "    \"I structure projects so models can be reused efficiently\",\n",
        "    \"I document my code and workflows for clarity\",\n",
        "    \"I sometimes print words with pauses to mimic human thinking\",\n",
        "    \"I combine software engineering best practices with ML experiments\",\n",
        "    \"I have worked on Spring Batch jobs for data processing\",\n",
        "    \"I consolidate legacy applications into modern architectures\",\n",
        "    \"I use Elasticsearch and Kibana for monitoring data pipelines\",\n",
        "    \"I optimize CI/CD with Jenkins, Gradle, and Maven\",\n",
        "    \"I test APIs using Postman and debug microservices\",\n",
        "    \"I actively participate in solution design and system architecture\",\n",
        "    \"I discuss system needs with stakeholders and product owners\",\n",
        "    \"I explore OCR and PDF processing pipelines\",\n",
        "    \"I like reading articles and summarizing content using RAG\",\n",
        "    \"I explore embeddings, vector databases, and text retrieval\",\n",
        "    \"I have experimented with auto-completing and decoding sentences\",\n",
        "    \"I practice English with PTE preparation exercises\",\n",
        "    \"I explore Australian visa options and state sponsorship details\",\n",
        "    \"I track my fitness with chia seeds, oats, and milk\",\n",
        "    \"I like adding dark chocolate or kitul syrup to oats\",\n",
        "    \"I go for walks and sometimes take the dog along\",\n",
        "    \"I enjoy simple meals that are tasty and healthy\",\n",
        "    \"I roast myself with one-line jokes for fun\",\n",
        "    \"I debug Airflow DAGs and monitor pipeline runs\",\n",
        "    \"I handle S3 buckets and manage object events\",\n",
        "    \"I explore localstack for AWS testing in my setup\",\n",
        "    \"I like structuring pipelines with parent and downstream tasks\",\n",
        "    \"I generate pipeline IDs and logs for debugging purposes\",\n",
        "    \"I research NLP papers and understand transformers deeply\",\n",
        "    \"I experiment with masked attention and QKV matrices\",\n",
        "    \"I calculate embeddings step by step for learning\",\n",
        "    \"I sometimes code tiny neural networks from scratch\",\n",
        "    \"I explore word2vec and embeddings in Python for NLP\",\n",
        "    \"I enjoy building RAG-based search systems for documents\",\n",
        "    \"I debug errors and optimize batch jobs carefully\",\n",
        "    \"I write modular service classes for data reading and writing\",\n",
        "    \"I experiment with chunked data processing and retry logic\",\n",
        "    \"I take ownership of coding tasks and deliver solutions\",\n",
        "    \"I plan my sprints and track tasks in Jira\",\n",
        "    \"I collaborate with tech leads and solution architects\",\n",
        "    \"I explore advanced AI topics while staying hands-on with code\",\n",
        "    \"I combine learning and building small projects iteratively\",\n",
        "    \"I like generating sentences that sound natural and human-like\"\n",
        "]"
      ],
      "metadata": {
        "id": "eCiOQOBq7_94"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text = \" \".join(sentences).lower().split()\n",
        "tokens = list(set(all_text)) + [\"<EOS>\"]"
      ],
      "metadata": {
        "id": "veoDtBCo8GL1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token2id = {tok: idx for idx, tok in enumerate(sorted(tokens))}\n",
        "id2token = {idx: tok for tok, idx in token2id.items()}\n",
        "vocab_size = len(token2id)\n",
        "print(token2id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy4WZx148NnV",
        "outputId": "0441320a-c8ee-462b-f33c-9e6e8eff4eff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<EOS>': 0, 'a': 1, 'actively': 2, 'actually': 3, 'adding': 4, 'advanced': 5, 'ai': 6, 'airflow': 7, 'along': 8, 'analyze': 9, 'and': 10, 'apis': 11, 'applications': 12, 'architects': 13, 'architecture': 14, 'architectures': 15, 'are': 16, 'articles': 17, 'at': 18, 'attention': 19, 'australian': 20, 'auto-completing': 21, 'automate': 22, 'aws': 23, 'basis': 24, 'batch': 25, 'be': 26, 'best': 27, 'buckets': 28, 'build': 29, 'building': 30, 'by': 31, 'calculate': 32, 'can': 33, 'capacity': 34, 'capture': 35, 'carefully': 36, 'checking': 37, 'chia': 38, 'chocolate': 39, 'chunked': 40, 'ci/cd': 41, 'clarity': 42, 'classes': 43, 'clear': 44, 'code': 45, 'coding': 46, 'colab': 47, 'collaborate': 48, 'combine': 49, 'consolidate': 50, 'content': 51, 'context': 52, 'control': 53, 'creative': 54, 'd_model': 55, 'dags': 56, 'dark': 57, 'data': 58, 'databases,': 59, 'datasets': 60, 'debug': 61, 'debugging': 62, 'decoding': 63, 'deeply': 64, 'deliver': 65, 'design': 66, 'details': 67, 'different': 68, 'discuss': 69, 'diverges': 70, 'docker': 71, 'document': 72, 'documents': 73, 'dog': 74, 'downstream': 75, 'each': 76, 'easier': 77, 'efficiency': 78, 'efficiently': 79, 'elasticsearch': 80, 'embedding': 81, 'embeddings': 82, 'embeddings,': 83, 'encoding': 84, 'engineering': 85, 'english': 86, 'enjoy': 87, 'enlarge': 88, 'errors': 89, 'events': 90, 'exercises': 91, 'experiment': 92, 'experimented': 93, 'experiments': 94, 'explore': 95, 'fails': 96, 'feed': 97, 'first': 98, 'fitness': 99, 'focus': 100, 'for': 101, 'forward': 102, 'from': 103, 'fun': 104, 'generate': 105, 'generating': 106, 'generation': 107, 'github': 108, 'go': 109, 'gradle,': 110, 'grow': 111, 'handle': 112, 'hands-on': 113, 'have': 114, 'healthy': 115, 'help': 116, 'human': 117, 'human-like': 118, 'i': 119, 'ids': 120, 'improve': 121, 'in': 122, 'integration': 123, 'into': 124, 'is': 125, 'iterative': 126, 'iteratively': 127, 'jenkins': 128, 'jenkins,': 129, 'jira': 130, 'jobs': 131, 'jokes': 132, 'kibana': 133, 'kitul': 134, 'layers': 135, 'leads': 136, 'learning': 137, 'legacy': 138, 'lets': 139, 'like': 140, 'localstack': 141, 'logging': 142, 'logic': 143, 'logs': 144, 'look': 145, 'loops': 146, 'make': 147, 'makes': 148, 'making': 149, 'manage': 150, 'masked': 151, 'matrices': 152, 'maven': 153, 'meals': 154, 'meaning': 155, 'microservices': 156, 'milk': 157, 'mimic': 158, 'mini': 159, 'ml': 160, 'model': 161, 'models': 162, 'modern': 163, 'modular': 164, 'monitor': 165, 'monitoring': 166, 'more': 167, 'my': 168, 'myself': 169, 'natural': 170, 'needs': 171, 'networks': 172, 'neural': 173, 'next-word': 174, 'nlp': 175, 'notebooks': 176, 'oats': 177, 'oats,': 178, 'object': 179, 'ocr': 180, 'of': 181, 'often': 182, 'on': 183, 'one': 184, 'one-line': 185, 'optimize': 186, 'options': 187, 'or': 188, 'order': 189, 'other': 190, 'outputs': 191, 'owners': 192, 'ownership': 193, 'papers': 194, 'parent': 195, 'participate': 196, 'pauses': 197, 'pdf': 198, 'pipeline': 199, 'pipelines': 200, 'plan': 201, 'positional': 202, 'postman': 203, 'practice': 204, 'practices': 205, 'prediction': 206, 'preparation': 207, 'print': 208, 'processing': 209, 'produce': 210, 'product': 211, 'projects': 212, 'prompts': 213, 'prototypes': 214, 'pte': 215, 'purposes': 216, 'python': 217, 'pytorch': 218, 'qkv': 219, 'rag': 220, 'rag-based': 221, 'rates': 222, 'reading': 223, 'refine': 224, 'reflect': 225, 'representations': 226, 'research': 227, 'retrieval': 228, 'retry': 229, 'reused': 230, 'roast': 231, 'runs': 232, 'runtime': 233, 's3': 234, 'sampling': 235, 'scratch': 236, 'search': 237, 'seeds,': 238, 'self': 239, 'self-attention': 240, 'sentences': 241, 'sequences': 242, 'service': 243, 'setup': 244, 'shapes': 245, 'sharing': 246, 'short': 247, 'simple': 248, 'simulate': 249, 'sizes': 250, 'small': 251, 'so': 252, 'software': 253, 'solution': 254, 'solutions': 255, 'sometimes': 256, 'sound': 257, 'sponsorship': 258, 'spring': 259, 'sprints': 260, 'stabilize': 261, 'stakeholders': 262, 'state': 263, 'staying': 264, 'step': 265, 'structure': 266, 'structuring': 267, 'style': 268, 'summarizing': 269, 'synthetic': 270, 'syrup': 271, 'system': 272, 'systems': 273, 'take': 274, 'tasks': 275, 'tasty': 276, 'tech': 277, 'tells': 278, 'temperature': 279, 'tensor': 280, 'test': 281, 'testing': 282, 'text': 283, 'that': 284, 'the': 285, 'thinking': 286, 'time': 287, 'tiny': 288, 'to': 289, 'token': 290, 'top-k': 291, 'topics': 292, 'track': 293, 'training': 294, 'transformers': 295, 'transformers,': 296, 'tweak': 297, 'understand': 298, 'understanding': 299, 'use': 300, 'using': 301, 'vector': 302, 'vectors': 303, 'version': 304, 'visa': 305, 'walks': 306, 'want': 307, 'when': 308, 'while': 309, 'with': 310, 'word': 311, 'word2vec': 312, 'words': 313, 'worked': 314, 'workflows': 315, 'write': 316, 'writing': 317}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sentences to sequences of IDs\n",
        "sequences = [[token2id[word] for word in sentence.lower().split()] for sentence in sentences]\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf7sZzRI8Wib",
        "outputId": "1e8d26e0-c414-46ba-bcd9-a7d76a027f44"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[119, 140, 30, 251, 295, 289, 298, 175], [62, 45, 125, 77, 308, 142, 125, 44], [119, 307, 289, 105, 241, 290, 31, 290], [81, 303, 116, 313, 35, 155, 122, 52], [202, 84, 278, 285, 161, 285, 189, 181, 313], [119, 182, 92, 310, 218, 10, 47, 176], [108, 123, 148, 304, 53, 10, 246, 77], [119, 281, 168, 162, 183, 288, 60, 98], [239, 19, 139, 76, 311, 145, 18, 190, 313], [97, 102, 172, 224, 290, 226, 122, 295], [119, 297, 55, 10, 135, 289, 121, 161, 34], [174, 206, 125, 285, 24, 181, 283, 107], [119, 87, 149, 251, 6, 94, 284, 225, 168, 268], [119, 140, 126, 146, 289, 111, 241, 184, 290, 18, 1, 287], [279, 10, 291, 235, 147, 191, 167, 54], [119, 9, 144, 36, 308, 294, 96, 188, 70], [119, 186, 25, 250, 10, 137, 222, 101, 78], [119, 95, 270, 241, 289, 88, 288, 60], [119, 307, 289, 29, 1, 159, 6, 304, 181, 169], [119, 100, 183, 299, 296, 83, 10, 240], [119, 256, 316, 45, 289, 249, 286, 122, 107, 146], [119, 300, 128, 10, 71, 289, 22, 41, 200], [119, 140, 289, 92, 310, 247, 213, 10, 242], [119, 61, 233, 89, 31, 37, 280, 245], [119, 281, 68, 137, 222, 289, 261, 294], [119, 87, 30, 214, 284, 3, 210, 241], [119, 266, 212, 252, 162, 33, 26, 230, 79], [119, 72, 168, 45, 10, 315, 101, 42], [119, 256, 208, 313, 310, 197, 289, 158, 117, 286], [119, 49, 253, 85, 27, 205, 310, 160, 94], [119, 114, 314, 183, 259, 25, 131, 101, 58, 209], [119, 50, 138, 12, 124, 163, 15], [119, 300, 80, 10, 133, 101, 166, 58, 200], [119, 186, 41, 310, 129, 110, 10, 153], [119, 281, 11, 301, 203, 10, 61, 156], [119, 2, 196, 122, 254, 66, 10, 272, 14], [119, 69, 272, 171, 310, 262, 10, 211, 192], [119, 95, 180, 10, 198, 209, 200], [119, 140, 223, 17, 10, 269, 51, 301, 220], [119, 95, 83, 302, 59, 10, 283, 228], [119, 114, 93, 310, 21, 10, 63, 241], [119, 204, 86, 310, 215, 207, 91], [119, 95, 20, 305, 187, 10, 263, 258, 67], [119, 293, 168, 99, 310, 38, 238, 178, 10, 157], [119, 140, 4, 57, 39, 188, 134, 271, 289, 177], [119, 109, 101, 306, 10, 256, 274, 285, 74, 8], [119, 87, 248, 154, 284, 16, 276, 10, 115], [119, 231, 169, 310, 185, 132, 101, 104], [119, 61, 7, 56, 10, 165, 199, 232], [119, 112, 234, 28, 10, 150, 179, 90], [119, 95, 141, 101, 23, 282, 122, 168, 244], [119, 140, 267, 200, 310, 195, 10, 75, 275], [119, 105, 199, 120, 10, 144, 101, 62, 216], [119, 227, 175, 194, 10, 298, 295, 64], [119, 92, 310, 151, 19, 10, 219, 152], [119, 32, 82, 265, 31, 265, 101, 137], [119, 256, 45, 288, 173, 172, 103, 236], [119, 95, 312, 10, 82, 122, 217, 101, 175], [119, 87, 30, 221, 237, 273, 101, 73], [119, 61, 89, 10, 186, 25, 131, 36], [119, 316, 164, 243, 43, 101, 58, 223, 10, 317], [119, 92, 310, 40, 58, 209, 10, 229, 143], [119, 274, 193, 181, 46, 275, 10, 65, 255], [119, 201, 168, 260, 10, 293, 275, 122, 130], [119, 48, 310, 277, 136, 10, 254, 13], [119, 95, 5, 6, 292, 309, 264, 113, 310, 45], [119, 49, 137, 10, 30, 251, 212, 127], [119, 140, 106, 241, 284, 257, 170, 10, 118]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input/output pairs for next-word prediction\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    tokens = [token2id[w] for w in sentence.lower().split()] + [token2id[\"<EOS>\"]]\n",
        "    for i in range(1, len(tokens)):\n",
        "        X.append(tokens[:i])\n",
        "        Y.append(tokens[i])\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ngV6OXSD8esg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Positional Encoding\n",
        "\n",
        "In transformers, unlike RNNs, the model doesn‚Äôt inherently know the order of tokens in a sequence. To give it a sense of position, we add positional encodings to the input embeddings. These encodings help the model distinguish between the first word, second word, etc.\n",
        "\n",
        "Here‚Äôs what your code does:\n",
        "\n",
        "Parameters:\n",
        "\n",
        "d_model = 16 ‚Üí The dimensionality of the embeddings (number of features per token).\n",
        "\n",
        "max_len = 10 ‚Üí Maximum length of the sequence we want to encode positions for.\n",
        "\n",
        "Function get_positional_encoding(seq_len, d_model):\n",
        "\n",
        "Creates a zero tensor of shape (seq_len, d_model) to store the positional encodings.\n",
        "\n",
        "Loops over each position in the sequence (pos) and each dimension of the embedding (i).\n",
        "\n",
        "Even indices (i): Use the sine function\n",
        "\n",
        "ùëÉ\n",
        "ùê∏\n",
        "[\n",
        "ùëù\n",
        "ùëú\n",
        "ùë†\n",
        ",\n",
        "ùëñ\n",
        "]\n",
        "=\n",
        "sin\n",
        "‚Å°\n",
        "(\n",
        "ùëù\n",
        "ùëú\n",
        "ùë†\n",
        "10000\n",
        "ùëñ\n",
        "/\n",
        "ùëë\n",
        "_\n",
        "ùëö\n",
        "ùëú\n",
        "ùëë\n",
        "ùëí\n",
        "ùëô\n",
        ")\n",
        "PE[pos,i]=sin(\n",
        "10000\n",
        "i/d_model\n",
        "pos\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "\n",
        "Odd indices (i+1): Use the cosine function\n",
        "\n",
        "ùëÉ\n",
        "ùê∏\n",
        "[\n",
        "ùëù\n",
        "ùëú\n",
        "ùë†\n",
        ",\n",
        "ùëñ\n",
        "+\n",
        "1\n",
        "]\n",
        "=\n",
        "cos\n",
        "‚Å°\n",
        "(\n",
        "ùëù\n",
        "ùëú\n",
        "ùë†\n",
        "10000\n",
        "ùëñ\n",
        "/\n",
        "ùëë\n",
        "_\n",
        "ùëö\n",
        "ùëú\n",
        "ùëë\n",
        "ùëí\n",
        "ùëô\n",
        ")\n",
        "PE[pos,i+1]=cos(\n",
        "10000\n",
        "i/d_model\n",
        "pos\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "\n",
        "Why sine and cosine?\n",
        "\n",
        "They create a unique pattern for each position across all embedding dimensions.\n",
        "\n",
        "These patterns are continuous, so the model can infer the relative distances between positions.\n",
        "\n",
        "Using different frequencies (10000^(i/d_model)) ensures each dimension has a different periodicity.\n",
        "\n",
        "Output:\n",
        "\n",
        "pos_encoding is a tensor of shape (max_len, d_model).\n",
        "\n",
        "This tensor is later added to the input embeddings to inject positional information.\n",
        "\n",
        "üí° Intuition:\n",
        "Think of positional encoding as giving each token a ‚Äúlocation tag‚Äù in the sequence. Sine and cosine allow the model to figure out relative positions without hard-coding numbers."
      ],
      "metadata": {
        "id": "Z2gZ0wUN_mMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Step 2: Positional Encoding\n",
        "# ----------------------\n",
        "d_model = 32\n",
        "max_len = 12\n",
        "\n",
        "def get_positional_encoding(seq_len, d_model):\n",
        "    pe = torch.zeros(seq_len, d_model)\n",
        "    for pos in range(seq_len):\n",
        "        for i in range(0, d_model, 2):\n",
        "            pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n",
        "            if i + 1 < d_model:\n",
        "                pe[pos, i+1] = math.cos(pos / (10000 ** (i / d_model)))\n",
        "    return pe\n",
        "\n",
        "# pos_encoding = get_positional_encoding(max_len, d_model)"
      ],
      "metadata": {
        "id": "ABWALrPJ9VdU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Transformer Encoder Layer\n",
        "\n",
        "The transformer encoder layer is the core building block of a Transformer. It takes in a sequence of embeddings and outputs a transformed sequence that captures contextual relationships between tokens.\n",
        "\n",
        "Class Initialization (__init__)\n",
        "\n",
        "Linear projections for Q, K, V:\n",
        "\n",
        "self.W_q, self.W_k, self.W_v are linear layers that map the input embeddings into Query, Key, and Value vectors.\n",
        "\n",
        "Each has shape (d_model, d_model).\n",
        "\n",
        "These are the vectors used in self-attention to compute relationships between tokens.\n",
        "\n",
        "Feed-Forward Network (ffn):\n",
        "\n",
        "A small MLP applied independently to each position.\n",
        "\n",
        "Two linear layers: first expands the dimension by 4√ó (d_model -> 4*d_model), then reduces it back (4*d_model -> d_model).\n",
        "\n",
        "Uses ReLU for non-linearity.\n",
        "\n",
        "Layer Normalization (ln1, ln2):\n",
        "\n",
        "Helps stabilize training by normalizing the inputs at each step.\n",
        "\n",
        "Applied after residual connections."
      ],
      "metadata": {
        "id": "gAJjEL_KAE3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Step 3: Transformer Encoder Layer\n",
        "# ----------------------\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model*4, d_model)\n",
        "        )\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "        scores = Q @ K.T / math.sqrt(d_model)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_out = attn_weights @ V\n",
        "\n",
        "        x = self.ln1(x + attn_out)\n",
        "        x = self.ln2(x + self.ffn(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "KmbEoe0A9han"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Step 4: Mini Transformer\n",
        "# ----------------------\n",
        "class MiniTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = get_positional_encoding(max_len, d_model)\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer(d_model) for _ in range(num_layers)])\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, seq_ids):\n",
        "        seq_len = seq_ids.size(0)\n",
        "        x = self.embedding(seq_ids) + self.pos_encoding[:seq_len]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        logits = self.output_layer(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "VuabQcqz9k1X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Step 5: Training Loop\n",
        "# ----------------------\n",
        "num_layers = 2\n",
        "learning_rate = 0.0001\n",
        "model = MiniTransformer(vocab_size, d_model, num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for seq, target in zip(X, Y):\n",
        "        seq_ids = torch.tensor(seq)\n",
        "        target_id = torch.tensor([target])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(seq_ids)\n",
        "        pred = logits[-1].unsqueeze(0)\n",
        "        loss = criterion(pred, target_id)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsOcZdmg9ocF",
        "outputId": "e2c4a388-ab72-48d7-86a3-c617e586c4e9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 492.3332\n",
            "Epoch 20, Loss: 376.6899\n",
            "Epoch 30, Loss: 327.7786\n",
            "Epoch 40, Loss: 308.1327\n",
            "Epoch 50, Loss: 288.6196\n",
            "Epoch 60, Loss: 286.6647\n",
            "Epoch 70, Loss: 281.3670\n",
            "Epoch 80, Loss: 282.7868\n",
            "Epoch 90, Loss: 308.1195\n",
            "Epoch 100, Loss: 297.9632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Step 6: Test Prediction\n",
        "# ----------------------\n",
        "test_seq = torch.tensor([token2id[w] for w in [\"i\", \"like\"]])\n",
        "logits = model(test_seq)\n",
        "pred_id = logits[-1].argmax().item()\n",
        "print(pred_id)\n",
        "print(\"Input: 'i like'\")\n",
        "print(\"Predicted next word:\", id2token[pred_id])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELRGXxkD9wB3",
        "outputId": "75446493-e121-4487-c529-8ea420785a06"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "106\n",
            "Input: 'i like'\n",
            "Predicted next word: generating\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import sys\n",
        "\n",
        "# ----------------------\n",
        "# Step 6: Generate Sentence with \"pausing\"\n",
        "# ----------------------\n",
        "test_seq = [\"fitness\", \"is\", \"like\"]\n",
        "max_gen_len = 20  # maximum tokens to generate\n",
        "\n",
        "generated = test_seq.copy()\n",
        "\n",
        "# Print the starting sequence\n",
        "for word in test_seq:\n",
        "    print(word, end=\" \", flush=True)\n",
        "    time.sleep(0.3)  # small pause for \"thinking\"\n",
        "\n",
        "for _ in range(max_gen_len):\n",
        "    seq_ids = torch.tensor([token2id[w] for w in generated])\n",
        "    logits = model(seq_ids)\n",
        "    pred_id = logits[-1].argmax().item()\n",
        "    next_word = id2token[pred_id]\n",
        "\n",
        "    # Print next word immediately\n",
        "    print(next_word, end=\" \", flush=True)\n",
        "    time.sleep(0.3)  # pause between words\n",
        "\n",
        "    generated.append(next_word)\n",
        "\n",
        "    if next_word == \"<EOS>\":\n",
        "        break\n",
        "\n",
        "print()  # final newline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqKcZqNnOvvu",
        "outputId": "0b4018dd-5499-4f2d-b7d2-8d926c085890"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fitness is like layers systems tensor tasty and writing <EOS> \n"
          ]
        }
      ]
    }
  ]
}